---
title: "AN 597- Module 15"
author: "Faye Harwell"
date: "November 7, 2017"
output: html_document
---

# Module 15

### Multiple Linear Regression- looks at the relationship between each of two or more continous predictor variabbles and a continuous repsonse variable while holding the effect of all other predictor variables constant

### ANCOVA- looking at the relationship between one or more continuous predictor variables and a continuous repsonse variable within each of one or more categorical groups

#### The ideal situation is that each variable does not correlate with one another at all (this would be 0 co-variance)

#### To experiment with multiple linear regressions, lets construct a dataset of random numbers 

```{r}
R = matrix(cbind(1, 0.8, -0.5, 0, 0.8, 1, -0.3, 0.3, -0.5, -0.3, 1, 0.6, 0, 
    0.3, 0.6, 1), nrow = 4)
n <- 1000
k <- 4
M <- NULL
V <- NULL
mu <- c(15, 40, 5, 23)  # vector of variable means
s <- c(5, 20, 4, 15)  # vector of variable SDs
for (i in 1:k) {
    V <- rnorm(n, mu[i], s[i])
    M <- cbind(M, V)
}
M <- matrix(M, nrow = n, ncol = k)
orig <- as.data.frame(M)
names(orig) = c("Y", "X1", "X2", "X3")
head(orig)
```

#### This function cor() lets you determine the co-variance. Remember, 1.0 is perfectly covariated and 0.0 is not correlated at all

```{r}
cor(orig)  # variables are uncorrelated
```
```{r}
plot(orig)  # does quick bivariate plots for each pair of variables; using `pairs(orig)` would do the same
```
#### Now, let’s normalize and standardize our variables by subtracting the relevant means and dividing by the standard deviation. This converts them to Z scores from a standard normal distribution.

```{r}
ms <- apply(orig, 2, FUN = "mean")  # returns a vector of means, where we are taking this across dimension 2 of the array 'orig'
ms
```

```{r}
sds <- apply(orig, 2, FUN = "sd")
sds
```
```{r}
normalized <- sweep(orig, 2, STATS = ms, FUN = "-")  # 2nd dimension is columns, removing array of means, function = subtract
normalized <- sweep(normalized, 2, STATS = sds, FUN = "/")  # 2nd dimension is columns, scaling by array of sds, function = divide
head(normalized)  # now a dataframe of Z scores
```
```{r}

M <- as.matrix(normalized)  # redefine M as our matrix of normalized variables
```

#### With apply() we apply a function to the specified margin of an array or matrix, and with sweep() we then perform whatever function is specified on all of the elements in an array specified by the given margin.

#### Now use Cholesky Decomposition... weird name...

```{r}
U = chol(R)
newM = M %*% U
new = as.data.frame(newM)
names(new) = c("Y", "X1", "X2", "X3")
cor(new)  # note that is correlation matrix is what we are aiming for!
```
```{r}
plot(orig)
```
```{r}
plot(new)
```

#### Finally, we can scale these back out to the mean and distribution of our original random variables. This is using the sweep function in reverse- the values are not standardized 

```{r}
df <- sweep(new, 2, STATS = sds, FUN = "*")  # scale back out to original mean...
df <- sweep(df, 2, STATS = ms, FUN = "+")  # and standard deviation
head(df)
```
```{r}
cor(df)
```
```{r}
plot(df)
```

###CHALLENGE: Start off by making some bivariate scatterplots in {ggplot2}. Then, using simple linear regression as implemented with lm(), how does the response variable (Y) vary with each predictor variable (X1, X2, X3)? Are the β1 coefficients significant? How much of the variation in YY does each predictor explain in a simple bivariate linear model?

```{r}
library(ggplot2)
require(gridExtra)
```

```{r}
g1 <- ggplot(data = df, aes(x = X1, y = Y)) + geom_point() + geom_smooth(method = "lm", formula = y ~ x)
g2 <- ggplot(data = df, aes(x = X2, y = Y)) + geom_point() + geom_smooth(method = "lm",  formula = y ~ x)
g3 <- ggplot(data = df, aes(x = X3, y = Y)) + geom_point() + geom_smooth(method = "lm",  formula = y ~ x)
grid.arrange(g1, g2, g3, ncol = 3)
```

```{r}
m1 <- lm(data = df, formula = Y ~ X1)
summary(m1)
```
```{r}
m2 <- lm(data = df, formula = Y ~ X2)
summary(m2)
```
```{r}
m3 <- lm(data = df, formula = Y ~ X3)
summary(m3)
```

#### To review, with multiple regression, we are looking to model a response variable in terms of two or more predictor variables so we can evaluate the effect of several explanatory variables.

#### Using lm() and formula notation, we can fit a model with all three predictor variables. The + sign is used to add additional predictors to our model. 

```{r}
m <- lm(data = df, formula = Y ~ X1 + X2 + X3)
coef(m)
```
```{r}
summary(m)
```
```{r}
# let's check if our residuals are random normal...
plot(fitted(m), residuals(m))
```
#### You want your fitted model and residuals plot to show a random distribution. This means that all the other variation that your model is not accounting for is random (and not capable of being explained by some other non-accounted for variable).
```{r}
hist(residuals(m))
```
```{r}
qqnorm(residuals(m))
```
#### What does this output tell us? First off, the results of the omnibus F test tells us that the overall model is significant; using these three variables, we explain signficantly more of the variation in the response variable, Y, than we would using a model with just an intercept, i.e., just that Y = mean(Y).

#### You can calculate the F-statistic by hand:

```{r}
f <- (summary(m)$r.squared * (nrow(df) - (ncol(df) - 1) - 1))/((1 - summary(m)$r.squared) * 
    (ncol(df) - 1))
f
```
#### Second, looking at summary() we see that the ββ coefficient for each of our predictor variables (including X3X3) is significant. That is, each predictor is significant even when the effects of the other predictor variables are held constant. Recall that in the simple linear regression, the ββ coefficient for X3X3 was not significant.

#### Third, we can interpret our ββ coefficients as we did in simple linear regression… for each change of one unit in a particular predictor variable (holding the other predictors constant), our predicted value of the response variable changes ββ units.

### CHALLENGE: Load up the “zombies.csv” dataset again and run a linear model of height as a function of both weight and age. Is the overall model significant? Are both predictor variables significant when the other is controlled for?

```{r}
library(curl)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/zombies.csv")
z <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(z)
```

```{r}
m <- lm(data = z, height ~ weight + age)
summary(m)
```

#### We can use the same linear modeling approach to do analysis of covariance, where we have a continuous response variable and a combination of continuous and categorical predictor variables. Let’s return to our “zombies.csv” dataset and now include one continuous and one categorical variable in our model… we want to predict height as a function of age and gender, and we want to use Type II regression. What is our model formula?

```{r}
library(car)
m <- lm(data = z, formula = height ~ gender + age)
summary(m)
```
```{r}
m.aov <- Anova(m, type = "II")
m.aov # Use Type II Error
```

```{r}
plot(fitted(m), residuals(m))
```
```{r}
hist(residuals(m))
```
```{r}
qqnorm(residuals(m))
```
#### To visualize it...

```{r}
library(ggplot2)
p <- ggplot(data = z, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) + 
    scale_color_manual(values = c("goldenrod", "blue"))
p <- p + geom_abline(slope = m$coefficients[3], intercept = m$coefficients[1], 
    color = "goldenrod4")
p <- p + geom_abline(slope = m$coefficients[3], intercept = m$coefficients[1] + 
    m$coefficients[2], color = "darkblue")
p
```
#### Using the confint() function on our ANCOVA model results reveals the confidence intervals for each of the coefficients in our multiple regression, just as it did for simple regression.

```{r}
m <- lm(data = z, formula = height ~ age + gender)
summary(m)
```
```{r}
confint(m, level = 0.95)
```

#### Similarly, using predict() allows us to determine confidence intervals for the predicted mean response and prediction intervals for individual responses for a given combination of predictor variables.

#### So far, we have only considered the joint main effects of multiple predictors on a response variable, but often there are interactive effects between our predictors. An interactive effect is an additional change in the response that occurs because of particular combinations of predictors or because the relationship of one continuous variable to a response is contingent on a particular level of a categorical variable. We explored the former case a bit when we looked at ANOVAs involving two discrete predictors. Now, we’ll consider the latter case… is there an interactive effect of sex AND age on height in our population of zombie apocalypse survivors?

#### Using formula notation, it is easy for us to consider interactions between predictors. The colon (:) operator allows us to specify particular interactions we want to consider; we can use the asterisk (*) operator to specify a full model, i.e., all single terms factors and their interactions.

```{r}
m <- lm(data = z, height ~ age + gender + age:gender)  # or
summary(m)
```
```{r}
m <- lm(data = z, height ~ age * gender)
summary(m)
```
```{r}
coefficients(m)
```

### CHALLENGE: Load in the “KamilarAndCooper.csv”" dataset we have used previously. Reduce the dataset to the following variables: Family, Brain_Size_Female_Mean, Body_mass_female_mean, MeanGroupSize, DayLength_km, HomeRange_km2, and Move. Fit a Model I least squares multiple linear regression model using log(HomeRange_km2) as the response variable and log(Body_mass_female_mean), log(Brain_Size_Female_Mean), MeanGroupSize, and Move as predictor variables, and view a model summary.

```{r}
library(dplyr)
```
```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(d)
```
```{r}
d <- select(d, Brain_Size_Female_Mean, Family, Body_mass_female_mean, MeanGroupSize, 
    DayLength_km, HomeRange_km2, Move)
```

```{r}
m <- lm(data = d, log(HomeRange_km2) ~ log(Body_mass_female_mean) + log(Brain_Size_Female_Mean) + 
    MeanGroupSize + Move)
summary(m)
```
```{r}
plot(m$residuals)
```
```{r}
qqnorm(m$residuals)
```
#### Test for normality

```{r}
shapiro.test(m$residuals)
```
```{r}
m <- lm(data = d, log(HomeRange_km2) ~ log(Body_mass_female_mean) + log(Brain_Size_Female_Mean) + 
    MeanGroupSize)
summary(m)
```
```{r}
plot(m$residuals)
```
```{r}
qqnorm(m$residuals)
```
```{r}
shapiro.test(m$residuals)  # no significant deviation from normal
```
